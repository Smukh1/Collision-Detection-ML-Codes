{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e72f5c",
   "metadata": {},
   "source": [
    "# Using a smller chunk of the entire NGSIM dataset. Multivariate input and multivariate output time series problem has been defined and used to train a RNN model using LSTM and dense input layer and dense output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e809f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '5,6'\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5,6\"\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11224db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Use seaborn for pairplot.\n",
    "!pip install -q seaborn\n",
    "!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65c32ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 08:46:53.676349: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9360] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-23 08:46:53.676403: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-23 08:46:53.676455: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1537] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-23 08:46:53.687695: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Make NumPy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cd16f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tf.test.is_built_with_cuda())\n",
    "# print(tf.test.is_built_with_cudnn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b293abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to use\n",
    "columns_to_use = [\"global_x\", \"global_y\", \"v_class\", \"global_time\"] #using just 4 of them\n",
    "\n",
    "# Define chunk size for reading data\n",
    "chunk_size = 100\n",
    "\n",
    "# Initialize an empty list to store data chunks\n",
    "data_chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(\"https://data.transportation.gov/resource/8ect-6jqj.csv\", \n",
    "                         chunksize=chunk_size, usecols=columns_to_use):\n",
    "    data_chunks.append(chunk)\n",
    "\n",
    "# Concatenate data chunks into a single DataFrame\n",
    "trajectory_dataset = pd.concat(data_chunks, ignore_index=True)\n",
    "\n",
    "# trajectory_dataset = pd.read_csv(\n",
    "#     \"https://data.transportation.gov/resource/8ect-6jqj.csv\")\n",
    "\n",
    "#   names=[\"Vehicle_ID\", \"Frame_Id\", \"Total_Frames\", \"Global_Time\", \"Local_X\",\n",
    "#            \"Local_Y\", \"Global_X\", \"Global_Y\", \"v_length\", \"v_Width\", \"v_Vel\", \n",
    "#            \"v_Acc\", \"Lane_ID\", \"O_Zone\", \"D_Zone\", \"Int_ID\", \"Section_ID\", \"Direction\",\n",
    "#            \"Movement\", \"Preceding\", \"Following\", \"Space_Headway\", \"Time_Headway\", \"Location\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "182ad0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>global_time</th>\n",
       "      <th>global_x</th>\n",
       "      <th>global_y</th>\n",
       "      <th>v_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1163074800</td>\n",
       "      <td>2230522.629</td>\n",
       "      <td>1375574.155</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1163074100</td>\n",
       "      <td>2230522.629</td>\n",
       "      <td>1375574.155</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1163054800</td>\n",
       "      <td>2230522.629</td>\n",
       "      <td>1375574.155</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1163054300</td>\n",
       "      <td>2230522.629</td>\n",
       "      <td>1375574.155</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1163071000</td>\n",
       "      <td>2230522.629</td>\n",
       "      <td>1375574.155</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     global_time     global_x     global_y  v_class\n",
       "995   1163074800  2230522.629  1375574.155        2\n",
       "996   1163074100  2230522.629  1375574.155        2\n",
       "997   1163054800  2230522.629  1375574.155        2\n",
       "998   1163054300  2230522.629  1375574.155        2\n",
       "999   1163071000  2230522.629  1375574.155        2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_dataset = trajectory_dataset.copy()\n",
    "t_dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1072a866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Features Shape: (800, 4)\n",
      "Train Labels Shape: (800, 2)\n",
      "Test Features Shape: (200, 4)\n",
      "Test Labels Shape: (200, 2)\n"
     ]
    }
   ],
   "source": [
    "# Define input features and labels\n",
    "input_features = [\"global_x\", \"global_y\", \"v_class\", \"global_time\"]\n",
    "output_labels = [\"global_x\", \"global_y\"]\n",
    "\n",
    "# Split dataset into input features and labels\n",
    "X = t_dataset[input_features]\n",
    "y = t_dataset[output_labels]\n",
    "\n",
    "# Split the dataset into training and test sets (80-20 split)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shape of training and test sets\n",
    "print(\"Train Features Shape:\", train_features.shape)\n",
    "print(\"Train Labels Shape:\", train_labels.shape)\n",
    "print(\"Test Features Shape:\", test_features.shape)\n",
    "print(\"Test Labels Shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55ee9a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_x = train_labels['global_x']\n",
    "train_labels_y = train_labels['global_y']\n",
    "\n",
    "test_labels_x = test_labels['global_x']\n",
    "test_labels_y = test_labels['global_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e2797b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[[[2.231e+06 1.376e+06 2.067e+00 1.163e+09]]]]]]\n"
     ]
    }
   ],
   "source": [
    "input_normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "input_normalizer.adapt(np.array(train_features))\n",
    "print(input_normalizer.mean.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e72b8ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first = np.array(train_features[:1])\n",
    "\n",
    "# with np.printoptions(precision=2, suppress=True):\n",
    "#   print('First example:', first)\n",
    "#   print()\n",
    "#   print('Normalized:', input_normalizer(first).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c50470ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_normalized = input_normalizer(np.array(train_features)).numpy()\n",
    "\n",
    "test_features_normalized = input_normalizer(np.array(test_features)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f3d0b388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2230521.2 1375565.4]]\n"
     ]
    }
   ],
   "source": [
    "#normailizing the output layer\n",
    "output_normalizer=layers.Normalization(axis=-1)\n",
    "output_normalizer.adapt(train_labels)\n",
    "print(output_normalizer.mean.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9f45391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking the nomalized output layer\n",
    "# second = np.array(train_labels[:1])\n",
    "# print(output_normalizer(second).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "98692a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_normalized = output_normalizer(np.array(train_labels)).numpy()\n",
    "\n",
    "test_labels_normalized = output_normalizer(np.array(test_labels)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "079b2569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping train_features and test_features to include the timestep dimension (to ensure compatibility with the RNN layer's expected input shape)\n",
    "train_features = np.expand_dims(train_features, axis=1)\n",
    "test_features = np.expand_dims(test_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c78db554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining RNN model architecture\n",
    "rnn_model = keras.Sequential([\n",
    "    layers.LSTM(64, return_sequences=True, input_shape=(1, len(input_features))),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(len(output_labels))  # Output layer predicts global_x and global_y\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "rnn_model.compile(loss='mean_absolute_error',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b703d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# history = model.fit(train_dataset, validation_data=test_dataset, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b9042658",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 1, 4), found shape=(32, 1, 1, 1, 1, 4)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file3p75_63z.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 1, 4), found shape=(32, 1, 1, 1, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = rnn_model.fit(\n",
    "    train_features_normalized,\n",
    "    train_labels_normalized,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41f590c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 2ms/step - loss: 0.0000e+00\n",
      "Test Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "test_loss = rnn_model.evaluate(test_features)\n",
    "print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = rnn_model.evaluate(test_features, {'output_x': test_labels_x, 'output_y': test_labels_y}, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15e7c360",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_loss\u001b[49m(history)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_loss' is not defined"
     ]
    }
   ],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cbbe97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
